#Args
model_name: bert-base
dir: ./results/pretraining/bert-base
model:
  pretrained_model_name: bert-base-uncased
args:
  output_dir: ${dir}/ckpts
  logging_dir: ${dir}/runs
  per_device_train_batch_size: 1
  weight_decay: 0.01
  learning_rate: 1e-4
  num_train_epochs: 5
  adam_epsilon: 1e-5
  lr_scheduler_type: linear
  logging_first_step: true
  warmup_ratio: 0.1
  logging_steps: 30
  seed: 42
  dataloader_num_workers: 2

trainer:
  pretrained_tokenizer_name: bert-base-uncased
  save_model_name: ${dir}/bert-base-final