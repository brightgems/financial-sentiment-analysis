#Args
model_name: bert-base
dir: ./results/bert-base
model:
  pretrained_model_name: bert-base-uncased
args:
  output_dir: ${dir}/ckpts
  logging_dir: ${dir}/runs
  evaluation_strategy: steps
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  weight_decay: 0.01
  learning_rate: 1e-4
  num_train_epochs: 15
  adam_epsilon: 1e-6
  lr_scheduler_type: cosine_with_restarts
  logging_first_step: true
  warmup_ratio: 0.0
  logging_steps: 20
  save_steps: 100
  save_total_limit: 2
  eval_steps: 20
  seed: 42
  dataloader_num_workers: 2

trainer:
  pretrained_tokenizer_name: bert-base-uncased
  save_model_name: ${dir}/bert-base-final
misc:
  squad_v2: false
  raw_predictions_file: ${dir}/preds/raw
  acc_metric_file: ${dir}/preds/acc.txt
  f1_metric_file: ${dir}/preds/f1.txt
  final_predictions_file: ${dir}/preds/final
